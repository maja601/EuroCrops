{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Crop Classification using Eurocrops Data\n",
    "\n",
    "Model architecture adapted from [Garnot et al., 2020](https://openaccess.thecvf.com/content_CVPR_2020/papers/Garnot_Satellite_Image_Time_Series_Classification_With_Pixel-Set_Encoders_and_Temporal_CVPR_2020_paper.pdf)\n",
    "\n",
    "Additional Layerwise Relevance Propagation component from [Chan et al., 2023](https://ieeexplore.ieee.org/document/10281498)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using TinyEuroCrops, only the section below needs to be adjusted for desired variables and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training related parameters \n",
    "# number of epochs of training\n",
    "n_epochs = 10\n",
    "# size of the batches\n",
    "batch_size = 128\n",
    "# focal loss:  focusing parameter\n",
    "gamma = 1\n",
    "# adam: learning rate\n",
    "learning_rate = 1e-5\n",
    "# adam: decay of first order momentum of gradient\n",
    "b1 = 0.9\n",
    "# adam: decay of first order momentum of gradient\n",
    "b2 = 0.999\n",
    "# adam: weight decay (L2 penalty)\n",
    "weight_decay = 1e-6\n",
    "# print frequency of progress meter\n",
    "print_freq = 50\n",
    "\n",
    "\n",
    "# name of checkpoint \n",
    "CP_name= 'checkpoint.pth.tar'\n",
    "# initialize a dummy best accuracy \n",
    "best_acc1 = 0\n",
    "# location to save current tensorboard session\n",
    "current_run = '/Users/ayshahchan/Desktop/PhD/runs/psetae'\n",
    "\n",
    "# Data Directory\n",
    "# this section is specific to TinyEuroCrops, should other data be used please adjust the Data Loader Section for the correct directories as well\n",
    "# root location of data\n",
    "root = '/Users/ayshahchan/Desktop/Education/ESPACE/thesis/codes/data'\n",
    "partition = \"train\"\n",
    "# This notebook uses the train section of Austrian TinyEuroCrops\n",
    "country='AT_T33UWP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader for EuroCrop Demo Data TinyEuroCrops\n",
    "\n",
    "The data loader loads the data from 4 different files: one containing the spectral reflectances for training, one containing the spectral reflectances for testing, one containing the labels for training and the last containing the labels for testing.\n",
    "\n",
    "The code assumes the files are saved under different folders in the same root directory. Should the paths are different, please adjust the code in this section accordingly. This code assumes TinyEuroCrops file structure.\n",
    "\n",
    "\n",
    "Note: although the processing process is similar to the webinar codes, some column names are different. When applying this code on newly processed datasets: please take note the column names crpgrpc and crpgrpn may be hcat_c and hcat_n instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Model\n",
    "There are three main parts of the model: the multilayer perceptron encoder, the attention layer and the multilayer perceptron decoder. \n",
    "\n",
    "![model diagram](webinar_demo/model.png)\n",
    "![attention diagram](webinar_demo/attention_block.png)\n",
    "\n",
    "\n",
    "You can adjust the model as you wish, whether it is adding more layers or removing the dropout layers. Removing the dropout layers will lead to faster training but may also lead to overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer Perceptrons\n",
    "\n",
    "See mlps.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral Encoder\n",
    "\n",
    "A simple encoder for initial feature extraction mainly in the spectral domain.\n",
    "\n",
    "See pse.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Layer\n",
    "\n",
    "For Positional Encoding: one of the inputs is days. This refers to the number of days since first data point due to the irregular frequency of data acquisition. This should be a readily availble output of the data loader.\n",
    "\n",
    "See positional_encoding.py, attention_layer.py, lrp_ln.py\n",
    "\n",
    "Attention layer based on Garnot and Marc's implementation\n",
    "\n",
    "forward_lrp drops the dropout layer + detaches softmax and variance(in the layer norm) as per LRP propagation rules adapted from [Ali et al., 2022](https://arxiv.org/abs/2202.07304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal Attention Encoder\n",
    "\n",
    "Encoder that primarily extracts features in the temporal domain by leveraging the attention mechanism\n",
    "\n",
    "See tae.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all the individual modules\n",
    "\n",
    "See pse_tae.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Loss Function\n",
    "\n",
    "See focalloss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(current_run)\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from focalloss import FocalLoss\n",
    "from pse_tae import PSE_TAE\n",
    "from eurocrops_dataloader import EuroCropsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader,\n",
    "          pse_tae,\n",
    "          focal_loss,\n",
    "          optimizer,\n",
    "          epoch,\n",
    "          print_freq,\n",
    "          device):\n",
    "\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    # pse_time = AverageMeter('PSE', ':6.3f')\n",
    "    # tae_time = AverageMeter('TAE', ':6.3f')\n",
    "    # decode_time = AverageMeter('Decode', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@2', ':6.2f')\n",
    "    progress = ProgressMeter(len(train_loader),\n",
    "                             [batch_time, data_time, losses, top1, top5],\n",
    "                             prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # -------------------------\n",
    "    #  Put Models in Train Mode\n",
    "    # -------------------------\n",
    "    pse_tae.train()\n",
    "\n",
    "    end = time.time()\n",
    "    # running_loss = 0.0\n",
    "    # running_correct = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # Get training data\n",
    "        data = batch['data'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        days = batch[\"dates\"].to(device)\n",
    "        \n",
    "        \n",
    "        # ---------------------------------\n",
    "        #  Train Everything together\n",
    "        # ---------------------------------\n",
    "        optimizer.zero_grad()\n",
    "        output = pse_tae(data,days)\n",
    "        _, prediction = torch.max(output.data, 1)\n",
    "\n",
    "        # ---------------------------------\n",
    "        #  Loss\n",
    "        # ---------------------------------\n",
    "\n",
    "        # Focal Loss between output and target\n",
    "        loss = focal_loss(output.to(device), label)\n",
    "\n",
    "        # ---------------------------------\n",
    "        #  Record Stats\n",
    "        # ---------------------------------\n",
    "        # Measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, label, topk=(1, 2))\n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        top1.update(acc1[0], data.size(0))\n",
    "        top5.update(acc5[0], data.size(0))\n",
    "\n",
    "        # ---------------------------------\n",
    "        #  Gradient & SGD step\n",
    "        # ---------------------------------\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ---------------------------------\n",
    "        #  Time\n",
    "        # ---------------------------------\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            progress.display(i)\n",
    "            # for tensorboard\n",
    "            writer.add_scalar('train loss', loss.item(), epoch * len(train_loader) + i)\n",
    "            \n",
    "            writer.add_scalar('accuracy best', acc1, epoch * len(train_loader) + i)\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def validate(val_loader, pse_tae, focal_loss, epoch, print_freq, device):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(len(val_loader), [batch_time, losses, top1, top5], prefix='Test: ')\n",
    "\n",
    "    # -------------------------\n",
    "    #  Put Models in Eval Mode\n",
    "    # -------------------------\n",
    "    pse_tae.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, val_batch in enumerate(val_loader):\n",
    "            # Get validation data\n",
    "            data_val = val_batch['data'].to(device)\n",
    "            label_val = val_batch['label'].to(device)\n",
    "            days_val = val_batch[\"dates\"].to(device)\n",
    "     \n",
    "\n",
    "            # -------------------------\n",
    "            #  Compute Predictions\n",
    "            # -------------------------\n",
    "            output = pse_tae(data_val, days_val)\n",
    "            loss = focal_loss(output.to(device), label_val)\n",
    "            _, prediction = torch.max(output.data, 1)\n",
    "\n",
    "            # ---------------------------------\n",
    "            #  Record Stats\n",
    "            # ---------------------------------\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, label_val, topk=(1, 5))\n",
    "            losses.update(loss.item(), data_val.size(0))\n",
    "            top1.update(acc1[0], data_val.size(0))\n",
    "            top5.update(acc5[0], data_val.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                progress.display(i)\n",
    "                writer.add_scalar('val loss', loss.item(), epoch * len(val_loader) + i)\n",
    "            #writer.add_scalar('accuracy', running_correct/100, epoch * len(train_loader) + i)\n",
    "                writer.add_scalar('val accuracy', acc1, epoch * len(val_loader) + i)\n",
    "\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg, losses.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=CP_name):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'best_'+filename)\n",
    "\n",
    "def load_checkpoint(model, optimizer,  filename=CP_name):\n",
    "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
    "    start_epoch = 0\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, start_epoch\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0.2):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345970 parcels in file with 44 classes \n",
      "=> no checkpoint found at 'checkpoint.pth.tar'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][   0/2163]\tTime  1.982 ( 1.982)\tData  1.762 ( 1.762)\tLoss 3.7913e+00 (3.7913e+00)\tAcc@1   2.34 (  2.34)\tAcc@2   3.91 (  3.91)\n",
      "Epoch: [0][  50/2163]\tTime  0.239 ( 0.281)\tData  0.183 ( 0.218)\tLoss 3.7034e+00 (3.8050e+00)\tAcc@1   7.03 (  3.74)\tAcc@2  10.94 (  6.43)\n",
      "Epoch: [0][ 100/2163]\tTime  0.242 ( 0.263)\tData  0.184 ( 0.202)\tLoss 3.7096e+00 (3.7637e+00)\tAcc@1   5.47 (  4.56)\tAcc@2   9.38 (  7.80)\n",
      "Epoch: [0][ 150/2163]\tTime  0.241 ( 0.257)\tData  0.184 ( 0.197)\tLoss 3.6428e+00 (3.7221e+00)\tAcc@1   5.47 (  5.74)\tAcc@2  11.72 (  9.41)\n",
      "Epoch: [0][ 200/2163]\tTime  0.240 ( 0.254)\tData  0.184 ( 0.194)\tLoss 3.5550e+00 (3.6899e+00)\tAcc@1   7.81 (  6.72)\tAcc@2  12.50 ( 10.73)\n",
      "Epoch: [0][ 250/2163]\tTime  0.242 ( 0.252)\tData  0.185 ( 0.192)\tLoss 3.5681e+00 (3.6571e+00)\tAcc@1  10.16 (  7.88)\tAcc@2  16.41 ( 12.34)\n",
      "Epoch: [0][ 300/2163]\tTime  0.240 ( 0.251)\tData  0.184 ( 0.192)\tLoss 3.4271e+00 (3.6268e+00)\tAcc@1  19.53 (  9.09)\tAcc@2  24.22 ( 13.87)\n",
      "Epoch: [0][ 350/2163]\tTime  0.257 ( 0.250)\tData  0.200 ( 0.191)\tLoss 3.4175e+00 (3.6011e+00)\tAcc@1  16.41 ( 10.02)\tAcc@2  25.00 ( 15.05)\n",
      "Epoch: [0][ 400/2163]\tTime  0.247 ( 0.250)\tData  0.190 ( 0.190)\tLoss 3.3694e+00 (3.5757e+00)\tAcc@1  21.88 ( 10.94)\tAcc@2  28.12 ( 16.24)\n",
      "Epoch: [0][ 450/2163]\tTime  0.242 ( 0.249)\tData  0.184 ( 0.190)\tLoss 3.2605e+00 (3.5511e+00)\tAcc@1  24.22 ( 11.94)\tAcc@2  33.59 ( 17.47)\n",
      "Epoch: [0][ 500/2163]\tTime  0.240 ( 0.249)\tData  0.183 ( 0.189)\tLoss 3.2606e+00 (3.5279e+00)\tAcc@1  25.00 ( 12.77)\tAcc@2  30.47 ( 18.63)\n",
      "Epoch: [0][ 550/2163]\tTime  0.239 ( 0.248)\tData  0.183 ( 0.189)\tLoss 3.1921e+00 (3.5053e+00)\tAcc@1  28.12 ( 13.62)\tAcc@2  38.28 ( 19.82)\n",
      "Epoch: [0][ 600/2163]\tTime  0.242 ( 0.248)\tData  0.183 ( 0.189)\tLoss 3.2585e+00 (3.4835e+00)\tAcc@1  21.09 ( 14.36)\tAcc@2  32.03 ( 20.98)\n",
      "Epoch: [0][ 650/2163]\tTime  0.243 ( 0.248)\tData  0.183 ( 0.189)\tLoss 3.1621e+00 (3.4629e+00)\tAcc@1  27.34 ( 15.16)\tAcc@2  42.19 ( 22.13)\n",
      "Epoch: [0][ 700/2163]\tTime  0.248 ( 0.248)\tData  0.184 ( 0.188)\tLoss 3.1489e+00 (3.4436e+00)\tAcc@1  28.12 ( 15.89)\tAcc@2  35.16 ( 23.18)\n",
      "Epoch: [0][ 750/2163]\tTime  0.241 ( 0.247)\tData  0.183 ( 0.188)\tLoss 3.1000e+00 (3.4245e+00)\tAcc@1  32.81 ( 16.68)\tAcc@2  42.97 ( 24.28)\n",
      "Epoch: [0][ 800/2163]\tTime  0.242 ( 0.247)\tData  0.184 ( 0.188)\tLoss 3.1956e+00 (3.4058e+00)\tAcc@1  26.56 ( 17.46)\tAcc@2  36.72 ( 25.35)\n",
      "Epoch: [0][ 850/2163]\tTime  0.242 ( 0.247)\tData  0.184 ( 0.188)\tLoss 2.9933e+00 (3.3881e+00)\tAcc@1  35.94 ( 18.20)\tAcc@2  50.78 ( 26.35)\n",
      "Epoch: [0][ 900/2163]\tTime  0.242 ( 0.247)\tData  0.183 ( 0.188)\tLoss 3.1286e+00 (3.3711e+00)\tAcc@1  31.25 ( 18.90)\tAcc@2  47.66 ( 27.31)\n",
      "Epoch: [0][ 950/2163]\tTime  0.238 ( 0.247)\tData  0.180 ( 0.188)\tLoss 2.9647e+00 (3.3546e+00)\tAcc@1  35.16 ( 19.59)\tAcc@2  47.66 ( 28.30)\n",
      "Epoch: [0][1000/2163]\tTime  0.246 ( 0.247)\tData  0.188 ( 0.188)\tLoss 3.0554e+00 (3.3389e+00)\tAcc@1  39.84 ( 20.31)\tAcc@2  46.09 ( 29.25)\n",
      "Epoch: [0][1050/2163]\tTime  0.247 ( 0.246)\tData  0.188 ( 0.187)\tLoss 3.0616e+00 (3.3230e+00)\tAcc@1  31.25 ( 21.03)\tAcc@2  41.41 ( 30.20)\n",
      "Epoch: [0][1100/2163]\tTime  0.243 ( 0.246)\tData  0.185 ( 0.187)\tLoss 3.0372e+00 (3.3078e+00)\tAcc@1  31.25 ( 21.75)\tAcc@2  49.22 ( 31.09)\n",
      "Epoch: [0][1150/2163]\tTime  0.240 ( 0.246)\tData  0.182 ( 0.187)\tLoss 2.9578e+00 (3.2926e+00)\tAcc@1  37.50 ( 22.46)\tAcc@2  52.34 ( 31.97)\n",
      "Epoch: [0][1200/2163]\tTime  0.238 ( 0.246)\tData  0.180 ( 0.187)\tLoss 2.9817e+00 (3.2777e+00)\tAcc@1  33.59 ( 23.12)\tAcc@2  46.88 ( 32.85)\n",
      "Epoch: [0][1250/2163]\tTime  0.239 ( 0.245)\tData  0.184 ( 0.187)\tLoss 2.9220e+00 (3.2637e+00)\tAcc@1  37.50 ( 23.77)\tAcc@2  53.12 ( 33.67)\n",
      "Epoch: [0][1300/2163]\tTime  0.240 ( 0.245)\tData  0.186 ( 0.187)\tLoss 2.9404e+00 (3.2496e+00)\tAcc@1  40.62 ( 24.43)\tAcc@2  52.34 ( 34.50)\n",
      "Epoch: [0][1350/2163]\tTime  0.235 ( 0.245)\tData  0.181 ( 0.186)\tLoss 2.8527e+00 (3.2362e+00)\tAcc@1  42.97 ( 25.03)\tAcc@2  57.81 ( 35.28)\n",
      "Epoch: [0][1400/2163]\tTime  0.243 ( 0.245)\tData  0.187 ( 0.186)\tLoss 2.9144e+00 (3.2229e+00)\tAcc@1  39.06 ( 25.68)\tAcc@2  55.47 ( 36.05)\n",
      "Epoch: [0][1450/2163]\tTime  0.239 ( 0.245)\tData  0.181 ( 0.186)\tLoss 2.9401e+00 (3.2101e+00)\tAcc@1  38.28 ( 26.27)\tAcc@2  51.56 ( 36.79)\n",
      "Epoch: [0][1500/2163]\tTime  0.235 ( 0.245)\tData  0.180 ( 0.186)\tLoss 2.8638e+00 (3.1969e+00)\tAcc@1  40.62 ( 26.89)\tAcc@2  54.69 ( 37.54)\n",
      "Epoch: [0][1550/2163]\tTime  0.237 ( 0.245)\tData  0.180 ( 0.186)\tLoss 2.8429e+00 (3.1845e+00)\tAcc@1  43.75 ( 27.47)\tAcc@2  57.03 ( 38.23)\n",
      "Epoch: [0][1600/2163]\tTime  0.249 ( 0.244)\tData  0.193 ( 0.186)\tLoss 2.7999e+00 (3.1724e+00)\tAcc@1  41.41 ( 28.04)\tAcc@2  64.06 ( 38.92)\n",
      "Epoch: [0][1650/2163]\tTime  0.240 ( 0.244)\tData  0.185 ( 0.186)\tLoss 2.7710e+00 (3.1603e+00)\tAcc@1  42.19 ( 28.57)\tAcc@2  56.25 ( 39.58)\n",
      "Epoch: [0][1700/2163]\tTime  0.248 ( 0.245)\tData  0.191 ( 0.186)\tLoss 2.6889e+00 (3.1487e+00)\tAcc@1  49.22 ( 29.10)\tAcc@2  61.72 ( 40.21)\n",
      "Epoch: [0][1750/2163]\tTime  0.244 ( 0.245)\tData  0.188 ( 0.186)\tLoss 2.7609e+00 (3.1367e+00)\tAcc@1  46.09 ( 29.66)\tAcc@2  61.72 ( 40.84)\n",
      "Epoch: [0][1800/2163]\tTime  0.241 ( 0.245)\tData  0.184 ( 0.186)\tLoss 2.7291e+00 (3.1249e+00)\tAcc@1  44.53 ( 30.20)\tAcc@2  63.28 ( 41.48)\n",
      "Epoch: [0][1850/2163]\tTime  0.241 ( 0.245)\tData  0.186 ( 0.186)\tLoss 2.7054e+00 (3.1131e+00)\tAcc@1  52.34 ( 30.73)\tAcc@2  64.84 ( 42.11)\n",
      "Epoch: [0][1900/2163]\tTime  0.242 ( 0.245)\tData  0.185 ( 0.186)\tLoss 2.7105e+00 (3.1017e+00)\tAcc@1  46.88 ( 31.22)\tAcc@2  58.59 ( 42.69)\n",
      "Epoch: [0][1950/2163]\tTime  0.241 ( 0.245)\tData  0.184 ( 0.186)\tLoss 2.6557e+00 (3.0909e+00)\tAcc@1  51.56 ( 31.71)\tAcc@2  69.53 ( 43.25)\n",
      "Epoch: [0][2000/2163]\tTime  0.241 ( 0.245)\tData  0.184 ( 0.186)\tLoss 2.7041e+00 (3.0799e+00)\tAcc@1  42.19 ( 32.20)\tAcc@2  55.47 ( 43.79)\n",
      "Epoch: [0][2050/2163]\tTime  0.242 ( 0.245)\tData  0.183 ( 0.186)\tLoss 2.6421e+00 (3.0688e+00)\tAcc@1  53.12 ( 32.70)\tAcc@2  70.31 ( 44.34)\n",
      "Epoch: [0][2100/2163]\tTime  0.244 ( 0.244)\tData  0.186 ( 0.186)\tLoss 2.7358e+00 (3.0580e+00)\tAcc@1  42.19 ( 33.17)\tAcc@2  56.25 ( 44.87)\n",
      "Epoch: [0][2150/2163]\tTime  0.247 ( 0.244)\tData  0.187 ( 0.186)\tLoss 2.5859e+00 (3.0473e+00)\tAcc@1  55.47 ( 33.64)\tAcc@2  66.41 ( 45.39)\n",
      "Test: [  0/541]\tTime  0.210 ( 0.210)\tLoss 2.3178e+00 (2.3178e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  85.16 ( 85.16)\n",
      "Test: [ 50/541]\tTime  0.247 ( 0.224)\tLoss 2.4690e+00 (2.4189e+00)\tAcc@1  53.91 ( 57.87)\tAcc@5  81.25 ( 81.72)\n",
      "Test: [100/541]\tTime  0.214 ( 0.223)\tLoss 2.4253e+00 (2.4163e+00)\tAcc@1  57.03 ( 58.26)\tAcc@5  84.38 ( 82.01)\n",
      "Test: [150/541]\tTime  0.223 ( 0.227)\tLoss 2.3198e+00 (2.4143e+00)\tAcc@1  64.06 ( 58.59)\tAcc@5  84.38 ( 82.02)\n",
      "Test: [200/541]\tTime  0.211 ( 0.225)\tLoss 2.3086e+00 (2.4110e+00)\tAcc@1  63.28 ( 58.80)\tAcc@5  80.47 ( 82.00)\n",
      "Test: [250/541]\tTime  0.215 ( 0.224)\tLoss 2.3970e+00 (2.4135e+00)\tAcc@1  60.16 ( 58.65)\tAcc@5  80.47 ( 81.97)\n",
      "Test: [300/541]\tTime  0.216 ( 0.224)\tLoss 2.4776e+00 (2.4165e+00)\tAcc@1  57.81 ( 58.56)\tAcc@5  78.91 ( 81.93)\n",
      "Test: [350/541]\tTime  0.214 ( 0.224)\tLoss 2.2886e+00 (2.4163e+00)\tAcc@1  66.41 ( 58.54)\tAcc@5  84.38 ( 81.95)\n",
      "Test: [400/541]\tTime  0.210 ( 0.223)\tLoss 2.3252e+00 (2.4137e+00)\tAcc@1  61.72 ( 58.62)\tAcc@5  87.50 ( 82.09)\n",
      "Test: [450/541]\tTime  0.228 ( 0.223)\tLoss 2.3942e+00 (2.4142e+00)\tAcc@1  59.38 ( 58.51)\tAcc@5  83.59 ( 82.07)\n",
      "Test: [500/541]\tTime  0.220 ( 0.222)\tLoss 2.4589e+00 (2.4141e+00)\tAcc@1  57.03 ( 58.51)\tAcc@5  82.03 ( 82.10)\n",
      " * Acc@1 58.505 Acc@5 82.062\n",
      "Epoch: [1][   0/2163]\tTime  0.281 ( 0.281)\tData  0.219 ( 0.219)\tLoss 2.5936e+00 (2.5936e+00)\tAcc@1  55.47 ( 55.47)\tAcc@2  65.62 ( 65.62)\n",
      "Epoch: [1][  50/2163]\tTime  0.246 ( 0.255)\tData  0.188 ( 0.194)\tLoss 2.6727e+00 (2.5910e+00)\tAcc@1  53.91 ( 53.32)\tAcc@2  64.84 ( 67.14)\n",
      "Epoch: [1][ 100/2163]\tTime  0.248 ( 0.254)\tData  0.191 ( 0.193)\tLoss 2.5670e+00 (2.5772e+00)\tAcc@1  63.28 ( 53.69)\tAcc@2  71.09 ( 67.50)\n",
      "Epoch: [1][ 150/2163]\tTime  0.248 ( 0.256)\tData  0.190 ( 0.195)\tLoss 2.5906e+00 (2.5676e+00)\tAcc@1  57.03 ( 53.79)\tAcc@2  67.97 ( 68.02)\n",
      "Epoch: [1][ 200/2163]\tTime  0.249 ( 0.255)\tData  0.191 ( 0.194)\tLoss 2.4999e+00 (2.5572e+00)\tAcc@1  55.47 ( 54.16)\tAcc@2  66.41 ( 68.35)\n",
      "Epoch: [1][ 250/2163]\tTime  0.249 ( 0.255)\tData  0.192 ( 0.194)\tLoss 2.4597e+00 (2.5487e+00)\tAcc@1  61.72 ( 54.33)\tAcc@2  76.56 ( 68.56)\n",
      "Epoch: [1][ 300/2163]\tTime  0.249 ( 0.254)\tData  0.191 ( 0.193)\tLoss 2.5284e+00 (2.5392e+00)\tAcc@1  50.00 ( 54.50)\tAcc@2  67.97 ( 68.85)\n",
      "Epoch: [1][ 350/2163]\tTime  0.256 ( 0.255)\tData  0.191 ( 0.194)\tLoss 2.4046e+00 (2.5323e+00)\tAcc@1  65.62 ( 54.66)\tAcc@2  76.56 ( 68.95)\n",
      "Epoch: [1][ 400/2163]\tTime  0.260 ( 0.255)\tData  0.203 ( 0.194)\tLoss 2.4645e+00 (2.5274e+00)\tAcc@1  57.03 ( 54.83)\tAcc@2  71.09 ( 69.10)\n",
      "Epoch: [1][ 450/2163]\tTime  0.248 ( 0.255)\tData  0.188 ( 0.194)\tLoss 2.4966e+00 (2.5189e+00)\tAcc@1  53.12 ( 55.11)\tAcc@2  64.06 ( 69.36)\n",
      "Epoch: [1][ 500/2163]\tTime  0.244 ( 0.254)\tData  0.188 ( 0.193)\tLoss 2.4227e+00 (2.5111e+00)\tAcc@1  57.81 ( 55.40)\tAcc@2  73.44 ( 69.59)\n",
      "Epoch: [1][ 550/2163]\tTime  0.247 ( 0.254)\tData  0.186 ( 0.193)\tLoss 2.4741e+00 (2.5042e+00)\tAcc@1  50.00 ( 55.58)\tAcc@2  71.88 ( 69.72)\n",
      "Epoch: [1][ 600/2163]\tTime  0.255 ( 0.254)\tData  0.191 ( 0.193)\tLoss 2.4243e+00 (2.4964e+00)\tAcc@1  56.25 ( 55.80)\tAcc@2  71.09 ( 69.94)\n",
      "Epoch: [1][ 650/2163]\tTime  0.244 ( 0.253)\tData  0.187 ( 0.193)\tLoss 2.4391e+00 (2.4905e+00)\tAcc@1  63.28 ( 55.86)\tAcc@2  73.44 ( 69.98)\n",
      "Epoch: [1][ 700/2163]\tTime  0.251 ( 0.253)\tData  0.185 ( 0.192)\tLoss 2.3754e+00 (2.4841e+00)\tAcc@1  59.38 ( 55.97)\tAcc@2  71.88 ( 70.10)\n",
      "Epoch: [1][ 750/2163]\tTime  0.247 ( 0.253)\tData  0.187 ( 0.192)\tLoss 2.4337e+00 (2.4768e+00)\tAcc@1  54.69 ( 56.14)\tAcc@2  73.44 ( 70.29)\n",
      "Epoch: [1][ 800/2163]\tTime  0.252 ( 0.253)\tData  0.192 ( 0.192)\tLoss 2.3962e+00 (2.4700e+00)\tAcc@1  55.47 ( 56.26)\tAcc@2  71.09 ( 70.43)\n",
      "Epoch: [1][ 850/2163]\tTime  0.266 ( 0.253)\tData  0.206 ( 0.192)\tLoss 2.3871e+00 (2.4623e+00)\tAcc@1  57.81 ( 56.43)\tAcc@2  72.66 ( 70.58)\n",
      "Epoch: [1][ 900/2163]\tTime  0.253 ( 0.253)\tData  0.189 ( 0.192)\tLoss 2.4157e+00 (2.4557e+00)\tAcc@1  53.12 ( 56.58)\tAcc@2  75.78 ( 70.73)\n",
      "Epoch: [1][ 950/2163]\tTime  0.249 ( 0.253)\tData  0.189 ( 0.192)\tLoss 2.3957e+00 (2.4479e+00)\tAcc@1  57.03 ( 56.77)\tAcc@2  71.88 ( 70.90)\n",
      "Epoch: [1][1000/2163]\tTime  0.245 ( 0.253)\tData  0.185 ( 0.192)\tLoss 2.2428e+00 (2.4403e+00)\tAcc@1  64.06 ( 56.92)\tAcc@2  77.34 ( 71.08)\n",
      "Epoch: [1][1050/2163]\tTime  0.253 ( 0.253)\tData  0.192 ( 0.192)\tLoss 2.3820e+00 (2.4337e+00)\tAcc@1  54.69 ( 57.05)\tAcc@2  74.22 ( 71.24)\n",
      "Epoch: [1][1100/2163]\tTime  0.247 ( 0.253)\tData  0.189 ( 0.192)\tLoss 2.3161e+00 (2.4270e+00)\tAcc@1  62.50 ( 57.19)\tAcc@2  75.00 ( 71.36)\n",
      "Epoch: [1][1150/2163]\tTime  0.255 ( 0.253)\tData  0.192 ( 0.192)\tLoss 2.2453e+00 (2.4204e+00)\tAcc@1  63.28 ( 57.33)\tAcc@2  76.56 ( 71.48)\n",
      "Epoch: [1][1200/2163]\tTime  0.252 ( 0.253)\tData  0.189 ( 0.192)\tLoss 2.3421e+00 (2.4140e+00)\tAcc@1  50.78 ( 57.42)\tAcc@2  67.97 ( 71.58)\n",
      "Epoch: [1][1250/2163]\tTime  0.260 ( 0.254)\tData  0.199 ( 0.192)\tLoss 2.1783e+00 (2.4074e+00)\tAcc@1  62.50 ( 57.54)\tAcc@2  74.22 ( 71.69)\n",
      "Epoch: [1][1300/2163]\tTime  0.254 ( 0.254)\tData  0.194 ( 0.192)\tLoss 2.1098e+00 (2.4008e+00)\tAcc@1  65.62 ( 57.66)\tAcc@2  78.12 ( 71.79)\n",
      "Epoch: [1][1350/2163]\tTime  0.253 ( 0.254)\tData  0.194 ( 0.193)\tLoss 2.3276e+00 (2.3943e+00)\tAcc@1  55.47 ( 57.77)\tAcc@2  71.88 ( 71.91)\n",
      "Epoch: [1][1400/2163]\tTime  0.260 ( 0.254)\tData  0.202 ( 0.193)\tLoss 2.2028e+00 (2.3872e+00)\tAcc@1  60.16 ( 57.90)\tAcc@2  75.78 ( 72.04)\n",
      "Epoch: [1][1450/2163]\tTime  0.245 ( 0.254)\tData  0.189 ( 0.193)\tLoss 2.2312e+00 (2.3805e+00)\tAcc@1  65.62 ( 58.02)\tAcc@2  77.34 ( 72.15)\n",
      "Epoch: [1][1500/2163]\tTime  0.255 ( 0.254)\tData  0.190 ( 0.193)\tLoss 2.1233e+00 (2.3736e+00)\tAcc@1  63.28 ( 58.17)\tAcc@2  77.34 ( 72.29)\n",
      "Epoch: [1][1550/2163]\tTime  0.252 ( 0.254)\tData  0.190 ( 0.193)\tLoss 2.1745e+00 (2.3669e+00)\tAcc@1  62.50 ( 58.31)\tAcc@2  68.75 ( 72.38)\n",
      "Epoch: [1][1600/2163]\tTime  0.246 ( 0.254)\tData  0.189 ( 0.193)\tLoss 2.1844e+00 (2.3601e+00)\tAcc@1  64.06 ( 58.43)\tAcc@2  76.56 ( 72.47)\n",
      "Epoch: [1][1650/2163]\tTime  0.267 ( 0.254)\tData  0.210 ( 0.193)\tLoss 2.0552e+00 (2.3533e+00)\tAcc@1  64.84 ( 58.56)\tAcc@2  79.69 ( 72.58)\n",
      "Epoch: [1][1700/2163]\tTime  0.249 ( 0.254)\tData  0.190 ( 0.193)\tLoss 2.2320e+00 (2.3468e+00)\tAcc@1  56.25 ( 58.65)\tAcc@2  72.66 ( 72.67)\n",
      "Epoch: [1][1750/2163]\tTime  0.246 ( 0.254)\tData  0.190 ( 0.193)\tLoss 2.2211e+00 (2.3404e+00)\tAcc@1  63.28 ( 58.77)\tAcc@2  72.66 ( 72.76)\n",
      "Epoch: [1][1800/2163]\tTime  0.255 ( 0.254)\tData  0.193 ( 0.193)\tLoss 2.1223e+00 (2.3345e+00)\tAcc@1  59.38 ( 58.86)\tAcc@2  75.78 ( 72.85)\n",
      "Epoch: [1][1850/2163]\tTime  0.248 ( 0.254)\tData  0.187 ( 0.193)\tLoss 2.0686e+00 (2.3284e+00)\tAcc@1  59.38 ( 58.94)\tAcc@2  76.56 ( 72.93)\n",
      "Epoch: [1][1900/2163]\tTime  0.258 ( 0.254)\tData  0.197 ( 0.193)\tLoss 2.1252e+00 (2.3220e+00)\tAcc@1  58.59 ( 59.05)\tAcc@2  75.00 ( 73.03)\n",
      "Epoch: [1][1950/2163]\tTime  0.252 ( 0.254)\tData  0.193 ( 0.193)\tLoss 2.1039e+00 (2.3160e+00)\tAcc@1  57.03 ( 59.14)\tAcc@2  68.75 ( 73.11)\n",
      "Epoch: [1][2000/2163]\tTime  0.252 ( 0.255)\tData  0.195 ( 0.193)\tLoss 2.0629e+00 (2.3096e+00)\tAcc@1  63.28 ( 59.24)\tAcc@2  75.00 ( 73.19)\n",
      "Epoch: [1][2050/2163]\tTime  0.251 ( 0.255)\tData  0.192 ( 0.193)\tLoss 2.0868e+00 (2.3037e+00)\tAcc@1  62.50 ( 59.31)\tAcc@2  76.56 ( 73.24)\n",
      "Epoch: [1][2100/2163]\tTime  0.259 ( 0.255)\tData  0.189 ( 0.193)\tLoss 2.0556e+00 (2.2975e+00)\tAcc@1  60.94 ( 59.39)\tAcc@2  78.91 ( 73.32)\n",
      "Epoch: [1][2150/2163]\tTime  0.254 ( 0.255)\tData  0.198 ( 0.193)\tLoss 1.9371e+00 (2.2913e+00)\tAcc@1  68.75 ( 59.49)\tAcc@2  80.47 ( 73.42)\n",
      "Test: [  0/541]\tTime  0.211 ( 0.211)\tLoss 1.7197e+00 (1.7197e+00)\tAcc@1  75.00 ( 75.00)\tAcc@5  92.19 ( 92.19)\n",
      "Test: [ 50/541]\tTime  0.216 ( 0.213)\tLoss 2.0164e+00 (1.8386e+00)\tAcc@1  61.72 ( 66.87)\tAcc@5  85.16 ( 88.82)\n",
      "Test: [100/541]\tTime  0.224 ( 0.218)\tLoss 1.8287e+00 (1.8337e+00)\tAcc@1  70.31 ( 67.33)\tAcc@5  92.19 ( 89.09)\n",
      "Test: [150/541]\tTime  0.216 ( 0.219)\tLoss 1.7217e+00 (1.8356e+00)\tAcc@1  68.75 ( 67.43)\tAcc@5  89.06 ( 89.16)\n",
      "Test: [200/541]\tTime  0.219 ( 0.219)\tLoss 1.6970e+00 (1.8348e+00)\tAcc@1  69.53 ( 67.22)\tAcc@5  89.06 ( 89.09)\n",
      "Test: [250/541]\tTime  0.213 ( 0.219)\tLoss 1.8793e+00 (1.8370e+00)\tAcc@1  65.62 ( 67.02)\tAcc@5  86.72 ( 89.08)\n",
      "Test: [300/541]\tTime  0.212 ( 0.218)\tLoss 1.9222e+00 (1.8400e+00)\tAcc@1  64.84 ( 66.85)\tAcc@5  88.28 ( 89.06)\n",
      "Test: [350/541]\tTime  0.215 ( 0.219)\tLoss 1.7405e+00 (1.8398e+00)\tAcc@1  73.44 ( 66.71)\tAcc@5  89.84 ( 89.09)\n",
      "Test: [400/541]\tTime  0.213 ( 0.219)\tLoss 1.7698e+00 (1.8376e+00)\tAcc@1  70.31 ( 66.70)\tAcc@5  92.19 ( 89.15)\n",
      "Test: [450/541]\tTime  0.217 ( 0.219)\tLoss 1.8285e+00 (1.8387e+00)\tAcc@1  67.97 ( 66.61)\tAcc@5  92.19 ( 89.17)\n",
      "Test: [500/541]\tTime  0.212 ( 0.219)\tLoss 1.8788e+00 (1.8389e+00)\tAcc@1  70.31 ( 66.60)\tAcc@5  89.84 ( 89.20)\n",
      " * Acc@1 66.533 Acc@5 89.180\n",
      "Epoch: [2][   0/2163]\tTime  0.267 ( 0.267)\tData  0.202 ( 0.202)\tLoss 2.1522e+00 (2.1522e+00)\tAcc@1  57.81 ( 57.81)\tAcc@2  67.97 ( 67.97)\n",
      "Epoch: [2][  50/2163]\tTime  0.252 ( 0.255)\tData  0.195 ( 0.194)\tLoss 1.9220e+00 (2.0021e+00)\tAcc@1  67.19 ( 64.09)\tAcc@2  81.25 ( 76.78)\n",
      "Epoch: [2][ 100/2163]\tTime  0.246 ( 0.254)\tData  0.187 ( 0.193)\tLoss 2.0234e+00 (1.9965e+00)\tAcc@1  60.94 ( 64.46)\tAcc@2  75.78 ( 77.21)\n",
      "Epoch: [2][ 150/2163]\tTime  0.246 ( 0.253)\tData  0.189 ( 0.192)\tLoss 2.0477e+00 (1.9982e+00)\tAcc@1  61.72 ( 64.30)\tAcc@2  74.22 ( 77.08)\n",
      "Epoch: [2][ 200/2163]\tTime  0.249 ( 0.253)\tData  0.190 ( 0.192)\tLoss 2.0068e+00 (1.9957e+00)\tAcc@1  61.72 ( 64.27)\tAcc@2  76.56 ( 77.07)\n",
      "Epoch: [2][ 250/2163]\tTime  0.256 ( 0.253)\tData  0.198 ( 0.192)\tLoss 2.0432e+00 (1.9925e+00)\tAcc@1  56.25 ( 64.25)\tAcc@2  72.66 ( 77.03)\n",
      "Epoch: [2][ 300/2163]\tTime  0.246 ( 0.254)\tData  0.188 ( 0.192)\tLoss 2.0435e+00 (1.9865e+00)\tAcc@1  58.59 ( 64.33)\tAcc@2  73.44 ( 77.28)\n",
      "Epoch: [2][ 350/2163]\tTime  0.246 ( 0.254)\tData  0.188 ( 0.193)\tLoss 1.7494e+00 (1.9806e+00)\tAcc@1  79.69 ( 64.34)\tAcc@2  86.72 ( 77.37)\n",
      "Epoch: [2][ 400/2163]\tTime  0.246 ( 0.254)\tData  0.189 ( 0.192)\tLoss 1.8814e+00 (1.9755e+00)\tAcc@1  65.62 ( 64.33)\tAcc@2  80.47 ( 77.41)\n",
      "Epoch: [2][ 450/2163]\tTime  0.244 ( 0.253)\tData  0.187 ( 0.192)\tLoss 1.9689e+00 (1.9713e+00)\tAcc@1  63.28 ( 64.29)\tAcc@2  74.22 ( 77.44)\n",
      "Epoch: [2][ 500/2163]\tTime  0.245 ( 0.252)\tData  0.185 ( 0.192)\tLoss 1.8765e+00 (1.9648e+00)\tAcc@1  62.50 ( 64.37)\tAcc@2  78.12 ( 77.52)\n",
      "Epoch: [2][ 550/2163]\tTime  0.244 ( 0.252)\tData  0.187 ( 0.191)\tLoss 1.9509e+00 (1.9591e+00)\tAcc@1  60.16 ( 64.38)\tAcc@2  74.22 ( 77.59)\n",
      "Epoch: [2][ 600/2163]\tTime  0.243 ( 0.251)\tData  0.185 ( 0.191)\tLoss 1.9175e+00 (1.9536e+00)\tAcc@1  60.94 ( 64.44)\tAcc@2  76.56 ( 77.68)\n",
      "Epoch: [2][ 650/2163]\tTime  0.243 ( 0.251)\tData  0.186 ( 0.191)\tLoss 1.9309e+00 (1.9474e+00)\tAcc@1  67.19 ( 64.55)\tAcc@2  78.12 ( 77.75)\n",
      "Epoch: [2][ 700/2163]\tTime  0.249 ( 0.251)\tData  0.188 ( 0.190)\tLoss 1.7456e+00 (1.9430e+00)\tAcc@1  75.00 ( 64.57)\tAcc@2  87.50 ( 77.78)\n",
      "Epoch: [2][ 750/2163]\tTime  0.243 ( 0.250)\tData  0.184 ( 0.190)\tLoss 1.8906e+00 (1.9388e+00)\tAcc@1  64.84 ( 64.60)\tAcc@2  76.56 ( 77.82)\n",
      "Epoch: [2][ 800/2163]\tTime  0.253 ( 0.250)\tData  0.196 ( 0.190)\tLoss 1.8615e+00 (1.9340e+00)\tAcc@1  64.06 ( 64.64)\tAcc@2  78.91 ( 77.82)\n",
      "Epoch: [2][ 850/2163]\tTime  0.241 ( 0.250)\tData  0.184 ( 0.190)\tLoss 1.7540e+00 (1.9291e+00)\tAcc@1  72.66 ( 64.65)\tAcc@2  82.81 ( 77.87)\n",
      "Epoch: [2][ 900/2163]\tTime  0.260 ( 0.250)\tData  0.193 ( 0.190)\tLoss 1.7458e+00 (1.9237e+00)\tAcc@1  68.75 ( 64.71)\tAcc@2  82.03 ( 77.92)\n",
      "Epoch: [2][ 950/2163]\tTime  0.243 ( 0.250)\tData  0.185 ( 0.190)\tLoss 1.7519e+00 (1.9189e+00)\tAcc@1  65.62 ( 64.75)\tAcc@2  78.91 ( 77.95)\n",
      "Epoch: [2][1000/2163]\tTime  0.261 ( 0.250)\tData  0.195 ( 0.190)\tLoss 1.7320e+00 (1.9129e+00)\tAcc@1  73.44 ( 64.80)\tAcc@2  81.25 ( 78.02)\n",
      "Epoch: [2][1050/2163]\tTime  0.243 ( 0.250)\tData  0.185 ( 0.190)\tLoss 1.8149e+00 (1.9083e+00)\tAcc@1  60.16 ( 64.82)\tAcc@2  80.47 ( 78.07)\n",
      "Epoch: [2][1100/2163]\tTime  0.266 ( 0.250)\tData  0.203 ( 0.190)\tLoss 1.7351e+00 (1.9039e+00)\tAcc@1  67.97 ( 64.86)\tAcc@2  79.69 ( 78.06)\n",
      "Epoch: [2][1150/2163]\tTime  0.260 ( 0.251)\tData  0.200 ( 0.191)\tLoss 1.7248e+00 (1.8987e+00)\tAcc@1  65.62 ( 64.91)\tAcc@2  77.34 ( 78.11)\n",
      "Epoch: [2][1200/2163]\tTime  0.255 ( 0.251)\tData  0.196 ( 0.191)\tLoss 1.7718e+00 (1.8931e+00)\tAcc@1  69.53 ( 64.97)\tAcc@2  80.47 ( 78.16)\n",
      "Epoch: [2][1250/2163]\tTime  0.254 ( 0.251)\tData  0.192 ( 0.191)\tLoss 1.8309e+00 (1.8882e+00)\tAcc@1  60.16 ( 64.98)\tAcc@2  75.00 ( 78.19)\n",
      "Epoch: [2][1300/2163]\tTime  0.267 ( 0.251)\tData  0.204 ( 0.191)\tLoss 1.8082e+00 (1.8827e+00)\tAcc@1  63.28 ( 65.06)\tAcc@2  75.78 ( 78.25)\n",
      "Epoch: [2][1350/2163]\tTime  0.243 ( 0.251)\tData  0.184 ( 0.191)\tLoss 1.6825e+00 (1.8768e+00)\tAcc@1  66.41 ( 65.10)\tAcc@2  83.59 ( 78.30)\n",
      "Epoch: [2][1400/2163]\tTime  0.250 ( 0.251)\tData  0.189 ( 0.191)\tLoss 1.9837e+00 (1.8728e+00)\tAcc@1  59.38 ( 65.12)\tAcc@2  71.88 ( 78.31)\n",
      "Epoch: [2][1450/2163]\tTime  0.257 ( 0.252)\tData  0.192 ( 0.191)\tLoss 1.7153e+00 (1.8675e+00)\tAcc@1  67.97 ( 65.18)\tAcc@2  75.78 ( 78.35)\n",
      "Epoch: [2][1500/2163]\tTime  0.250 ( 0.252)\tData  0.192 ( 0.191)\tLoss 1.6903e+00 (1.8625e+00)\tAcc@1  70.31 ( 65.22)\tAcc@2  85.16 ( 78.41)\n",
      "Epoch: [2][1550/2163]\tTime  0.250 ( 0.252)\tData  0.190 ( 0.191)\tLoss 1.8530e+00 (1.8584e+00)\tAcc@1  58.59 ( 65.22)\tAcc@2  71.88 ( 78.43)\n",
      "Epoch: [2][1600/2163]\tTime  0.290 ( 0.252)\tData  0.209 ( 0.191)\tLoss 1.7652e+00 (1.8536e+00)\tAcc@1  52.34 ( 65.22)\tAcc@2  74.22 ( 78.44)\n",
      "Epoch: [2][1650/2163]\tTime  0.256 ( 0.252)\tData  0.201 ( 0.191)\tLoss 1.6597e+00 (1.8490e+00)\tAcc@1  68.75 ( 65.24)\tAcc@2  80.47 ( 78.46)\n",
      "Epoch: [2][1700/2163]\tTime  0.253 ( 0.252)\tData  0.187 ( 0.191)\tLoss 1.6960e+00 (1.8439e+00)\tAcc@1  65.62 ( 65.28)\tAcc@2  81.25 ( 78.53)\n",
      "Epoch: [2][1750/2163]\tTime  0.261 ( 0.252)\tData  0.199 ( 0.191)\tLoss 1.7181e+00 (1.8398e+00)\tAcc@1  67.19 ( 65.28)\tAcc@2  80.47 ( 78.56)\n",
      "Epoch: [2][1800/2163]\tTime  0.252 ( 0.252)\tData  0.186 ( 0.192)\tLoss 1.6338e+00 (1.8351e+00)\tAcc@1  66.41 ( 65.31)\tAcc@2  82.03 ( 78.58)\n",
      "Epoch: [2][1850/2163]\tTime  0.260 ( 0.252)\tData  0.197 ( 0.192)\tLoss 1.6180e+00 (1.8305e+00)\tAcc@1  68.75 ( 65.34)\tAcc@2  80.47 ( 78.62)\n",
      "Epoch: [2][1900/2163]\tTime  0.247 ( 0.252)\tData  0.186 ( 0.192)\tLoss 1.5869e+00 (1.8253e+00)\tAcc@1  69.53 ( 65.40)\tAcc@2  83.59 ( 78.67)\n",
      "Epoch: [2][1950/2163]\tTime  0.254 ( 0.253)\tData  0.195 ( 0.192)\tLoss 1.8020e+00 (1.8209e+00)\tAcc@1  60.16 ( 65.44)\tAcc@2  73.44 ( 78.70)\n",
      "Epoch: [2][2000/2163]\tTime  0.252 ( 0.253)\tData  0.193 ( 0.192)\tLoss 1.5695e+00 (1.8157e+00)\tAcc@1  66.41 ( 65.51)\tAcc@2  83.59 ( 78.76)\n",
      "Epoch: [2][2050/2163]\tTime  0.271 ( 0.253)\tData  0.188 ( 0.192)\tLoss 1.6643e+00 (1.8112e+00)\tAcc@1  63.28 ( 65.53)\tAcc@2  78.12 ( 78.79)\n",
      "Epoch: [2][2100/2163]\tTime  0.249 ( 0.253)\tData  0.190 ( 0.192)\tLoss 1.5020e+00 (1.8063e+00)\tAcc@1  67.19 ( 65.59)\tAcc@2  84.38 ( 78.85)\n",
      "Epoch: [2][2150/2163]\tTime  0.269 ( 0.253)\tData  0.206 ( 0.192)\tLoss 1.5256e+00 (1.8018e+00)\tAcc@1  69.53 ( 65.63)\tAcc@2  84.38 ( 78.89)\n",
      "Test: [  0/541]\tTime  0.214 ( 0.214)\tLoss 1.3005e+00 (1.3005e+00)\tAcc@1  75.78 ( 75.78)\tAcc@5  95.31 ( 95.31)\n",
      "Test: [ 50/541]\tTime  0.211 ( 0.217)\tLoss 1.6536e+00 (1.4341e+00)\tAcc@1  62.50 ( 69.82)\tAcc@5  87.50 ( 91.18)\n",
      "Test: [100/541]\tTime  0.210 ( 0.217)\tLoss 1.3681e+00 (1.4269e+00)\tAcc@1  71.09 ( 70.16)\tAcc@5  93.75 ( 91.37)\n",
      "Test: [150/541]\tTime  0.220 ( 0.217)\tLoss 1.3276e+00 (1.4291e+00)\tAcc@1  71.09 ( 70.26)\tAcc@5  91.41 ( 91.36)\n",
      "Test: [200/541]\tTime  0.214 ( 0.217)\tLoss 1.2957e+00 (1.4306e+00)\tAcc@1  72.66 ( 69.97)\tAcc@5  91.41 ( 91.33)\n",
      "Test: [250/541]\tTime  0.208 ( 0.217)\tLoss 1.5026e+00 (1.4335e+00)\tAcc@1  68.75 ( 69.79)\tAcc@5  89.06 ( 91.33)\n",
      "Test: [300/541]\tTime  0.212 ( 0.217)\tLoss 1.5160e+00 (1.4360e+00)\tAcc@1  64.84 ( 69.69)\tAcc@5  90.62 ( 91.31)\n",
      "Test: [350/541]\tTime  0.208 ( 0.217)\tLoss 1.3475e+00 (1.4358e+00)\tAcc@1  75.00 ( 69.63)\tAcc@5  90.62 ( 91.38)\n",
      "Test: [400/541]\tTime  0.208 ( 0.216)\tLoss 1.3723e+00 (1.4340e+00)\tAcc@1  76.56 ( 69.68)\tAcc@5  93.75 ( 91.43)\n",
      "Test: [450/541]\tTime  0.209 ( 0.215)\tLoss 1.4132e+00 (1.4352e+00)\tAcc@1  73.44 ( 69.63)\tAcc@5  94.53 ( 91.44)\n",
      "Test: [500/541]\tTime  0.211 ( 0.215)\tLoss 1.4481e+00 (1.4354e+00)\tAcc@1  71.09 ( 69.63)\tAcc@5  92.19 ( 91.45)\n",
      " * Acc@1 69.588 Acc@5 91.441\n",
      "Epoch: [3][   0/2163]\tTime  0.262 ( 0.262)\tData  0.194 ( 0.194)\tLoss 1.5858e+00 (1.5858e+00)\tAcc@1  64.84 ( 64.84)\tAcc@2  77.34 ( 77.34)\n",
      "Epoch: [3][  50/2163]\tTime  0.242 ( 0.246)\tData  0.186 ( 0.187)\tLoss 1.6080e+00 (1.5991e+00)\tAcc@1  65.62 ( 67.54)\tAcc@2  77.34 ( 80.36)\n",
      "Epoch: [3][ 100/2163]\tTime  0.244 ( 0.245)\tData  0.185 ( 0.187)\tLoss 1.6899e+00 (1.5917e+00)\tAcc@1  61.72 ( 67.54)\tAcc@2  79.69 ( 80.60)\n",
      "Epoch: [3][ 150/2163]\tTime  0.246 ( 0.245)\tData  0.186 ( 0.187)\tLoss 1.5840e+00 (1.5908e+00)\tAcc@1  72.66 ( 67.23)\tAcc@2  78.91 ( 80.43)\n",
      "Epoch: [3][ 200/2163]\tTime  0.245 ( 0.245)\tData  0.187 ( 0.187)\tLoss 1.5856e+00 (1.5854e+00)\tAcc@1  67.97 ( 67.29)\tAcc@2  82.81 ( 80.53)\n",
      "Epoch: [3][ 250/2163]\tTime  0.244 ( 0.245)\tData  0.185 ( 0.187)\tLoss 1.5646e+00 (1.5795e+00)\tAcc@1  64.06 ( 67.50)\tAcc@2  80.47 ( 80.65)\n",
      "Epoch: [3][ 300/2163]\tTime  0.243 ( 0.245)\tData  0.185 ( 0.187)\tLoss 1.5078e+00 (1.5729e+00)\tAcc@1  67.97 ( 67.59)\tAcc@2  83.59 ( 80.85)\n",
      "Epoch: [3][ 350/2163]\tTime  0.246 ( 0.245)\tData  0.184 ( 0.187)\tLoss 1.5897e+00 (1.5709e+00)\tAcc@1  64.06 ( 67.66)\tAcc@2  82.81 ( 80.93)\n",
      "Epoch: [3][ 400/2163]\tTime  0.243 ( 0.245)\tData  0.186 ( 0.187)\tLoss 1.6888e+00 (1.5657e+00)\tAcc@1  58.59 ( 67.72)\tAcc@2  71.09 ( 80.98)\n",
      "Epoch: [3][ 450/2163]\tTime  0.252 ( 0.245)\tData  0.186 ( 0.187)\tLoss 1.6264e+00 (1.5617e+00)\tAcc@1  57.03 ( 67.71)\tAcc@2  78.12 ( 81.01)\n",
      "Epoch: [3][ 500/2163]\tTime  0.243 ( 0.245)\tData  0.187 ( 0.187)\tLoss 1.5127e+00 (1.5584e+00)\tAcc@1  65.62 ( 67.70)\tAcc@2  79.69 ( 80.99)\n",
      "Epoch: [3][ 550/2163]\tTime  0.243 ( 0.245)\tData  0.186 ( 0.187)\tLoss 1.5990e+00 (1.5549e+00)\tAcc@1  67.19 ( 67.77)\tAcc@2  78.91 ( 81.00)\n",
      "Epoch: [3][ 600/2163]\tTime  0.246 ( 0.245)\tData  0.187 ( 0.187)\tLoss 1.5263e+00 (1.5512e+00)\tAcc@1  69.53 ( 67.80)\tAcc@2  82.03 ( 81.05)\n",
      "Epoch: [3][ 650/2163]\tTime  0.250 ( 0.246)\tData  0.191 ( 0.187)\tLoss 1.4768e+00 (1.5475e+00)\tAcc@1  68.75 ( 67.79)\tAcc@2  79.69 ( 81.07)\n",
      "Epoch: [3][ 700/2163]\tTime  0.240 ( 0.246)\tData  0.183 ( 0.187)\tLoss 1.4620e+00 (1.5438e+00)\tAcc@1  67.19 ( 67.86)\tAcc@2  82.81 ( 81.13)\n",
      "Epoch: [3][ 750/2163]\tTime  0.241 ( 0.246)\tData  0.184 ( 0.187)\tLoss 1.4527e+00 (1.5406e+00)\tAcc@1  67.97 ( 67.86)\tAcc@2  83.59 ( 81.11)\n",
      "Epoch: [3][ 800/2163]\tTime  0.247 ( 0.246)\tData  0.190 ( 0.187)\tLoss 1.4060e+00 (1.5367e+00)\tAcc@1  70.31 ( 67.94)\tAcc@2  84.38 ( 81.17)\n",
      "Epoch: [3][ 850/2163]\tTime  0.243 ( 0.246)\tData  0.185 ( 0.187)\tLoss 1.4107e+00 (1.5334e+00)\tAcc@1  68.75 ( 67.96)\tAcc@2  79.69 ( 81.17)\n",
      "Epoch: [3][ 900/2163]\tTime  0.247 ( 0.245)\tData  0.189 ( 0.187)\tLoss 1.5342e+00 (1.5296e+00)\tAcc@1  64.06 ( 67.97)\tAcc@2  75.00 ( 81.18)\n",
      "Epoch: [3][ 950/2163]\tTime  0.240 ( 0.245)\tData  0.184 ( 0.187)\tLoss 1.4185e+00 (1.5249e+00)\tAcc@1  74.22 ( 68.01)\tAcc@2  82.81 ( 81.23)\n",
      "Epoch: [3][1000/2163]\tTime  0.248 ( 0.246)\tData  0.190 ( 0.187)\tLoss 1.4436e+00 (1.5206e+00)\tAcc@1  71.88 ( 68.06)\tAcc@2  82.03 ( 81.28)\n",
      "Epoch: [3][1050/2163]\tTime  0.243 ( 0.246)\tData  0.186 ( 0.187)\tLoss 1.3229e+00 (1.5180e+00)\tAcc@1  73.44 ( 68.06)\tAcc@2  85.16 ( 81.28)\n",
      "Epoch: [3][1100/2163]\tTime  0.246 ( 0.246)\tData  0.189 ( 0.187)\tLoss 1.4442e+00 (1.5135e+00)\tAcc@1  67.19 ( 68.13)\tAcc@2  80.47 ( 81.32)\n",
      "Epoch: [3][1150/2163]\tTime  0.243 ( 0.246)\tData  0.187 ( 0.187)\tLoss 1.3675e+00 (1.5101e+00)\tAcc@1  72.66 ( 68.14)\tAcc@2  84.38 ( 81.34)\n",
      "Epoch: [3][1200/2163]\tTime  0.243 ( 0.246)\tData  0.186 ( 0.187)\tLoss 1.5181e+00 (1.5062e+00)\tAcc@1  61.72 ( 68.18)\tAcc@2  75.78 ( 81.35)\n",
      "Epoch: [3][1250/2163]\tTime  0.250 ( 0.247)\tData  0.192 ( 0.188)\tLoss 1.4667e+00 (1.5028e+00)\tAcc@1  64.06 ( 68.22)\tAcc@2  82.81 ( 81.38)\n",
      "Epoch: [3][1300/2163]\tTime  0.246 ( 0.247)\tData  0.187 ( 0.188)\tLoss 1.3277e+00 (1.4982e+00)\tAcc@1  74.22 ( 68.29)\tAcc@2  83.59 ( 81.44)\n",
      "Epoch: [3][1350/2163]\tTime  0.250 ( 0.247)\tData  0.189 ( 0.188)\tLoss 1.3807e+00 (1.4949e+00)\tAcc@1  71.09 ( 68.31)\tAcc@2  82.03 ( 81.47)\n",
      "Epoch: [3][1400/2163]\tTime  0.245 ( 0.247)\tData  0.186 ( 0.188)\tLoss 1.4286e+00 (1.4908e+00)\tAcc@1  67.97 ( 68.37)\tAcc@2  82.03 ( 81.51)\n",
      "Epoch: [3][1450/2163]\tTime  0.251 ( 0.247)\tData  0.189 ( 0.188)\tLoss 1.4206e+00 (1.4875e+00)\tAcc@1  71.09 ( 68.40)\tAcc@2  85.16 ( 81.53)\n",
      "Epoch: [3][1500/2163]\tTime  0.243 ( 0.247)\tData  0.185 ( 0.188)\tLoss 1.4804e+00 (1.4847e+00)\tAcc@1  63.28 ( 68.42)\tAcc@2  78.12 ( 81.52)\n",
      "Epoch: [3][1550/2163]\tTime  0.244 ( 0.247)\tData  0.185 ( 0.188)\tLoss 1.3320e+00 (1.4807e+00)\tAcc@1  70.31 ( 68.46)\tAcc@2  81.25 ( 81.57)\n",
      "Epoch: [3][1600/2163]\tTime  0.245 ( 0.247)\tData  0.187 ( 0.188)\tLoss 1.4230e+00 (1.4775e+00)\tAcc@1  64.84 ( 68.48)\tAcc@2  81.25 ( 81.58)\n",
      "Epoch: [3][1650/2163]\tTime  0.252 ( 0.247)\tData  0.190 ( 0.188)\tLoss 1.3720e+00 (1.4736e+00)\tAcc@1  66.41 ( 68.52)\tAcc@2  83.59 ( 81.61)\n",
      "Epoch: [3][1700/2163]\tTime  0.266 ( 0.248)\tData  0.190 ( 0.188)\tLoss 1.4153e+00 (1.4703e+00)\tAcc@1  65.62 ( 68.55)\tAcc@2  79.69 ( 81.63)\n",
      "Epoch: [3][1750/2163]\tTime  0.252 ( 0.248)\tData  0.192 ( 0.188)\tLoss 1.2680e+00 (1.4666e+00)\tAcc@1  77.34 ( 68.60)\tAcc@2  84.38 ( 81.68)\n",
      "Epoch: [3][1800/2163]\tTime  0.248 ( 0.248)\tData  0.188 ( 0.189)\tLoss 1.2564e+00 (1.4632e+00)\tAcc@1  74.22 ( 68.62)\tAcc@2  85.16 ( 81.71)\n",
      "Epoch: [3][1850/2163]\tTime  0.245 ( 0.248)\tData  0.188 ( 0.189)\tLoss 1.2240e+00 (1.4600e+00)\tAcc@1  75.78 ( 68.62)\tAcc@2  87.50 ( 81.74)\n",
      "Epoch: [3][1900/2163]\tTime  0.251 ( 0.248)\tData  0.193 ( 0.189)\tLoss 1.2512e+00 (1.4561e+00)\tAcc@1  73.44 ( 68.67)\tAcc@2  85.94 ( 81.79)\n",
      "Epoch: [3][1950/2163]\tTime  0.249 ( 0.248)\tData  0.190 ( 0.189)\tLoss 1.3470e+00 (1.4528e+00)\tAcc@1  66.41 ( 68.68)\tAcc@2  79.69 ( 81.81)\n",
      "Epoch: [3][2000/2163]\tTime  0.328 ( 0.248)\tData  0.258 ( 0.189)\tLoss 1.3721e+00 (1.4494e+00)\tAcc@1  67.97 ( 68.71)\tAcc@2  85.94 ( 81.84)\n",
      "Epoch: [3][2050/2163]\tTime  0.261 ( 0.249)\tData  0.203 ( 0.189)\tLoss 1.2530e+00 (1.4461e+00)\tAcc@1  72.66 ( 68.75)\tAcc@2  84.38 ( 81.85)\n",
      "Epoch: [3][2100/2163]\tTime  0.255 ( 0.249)\tData  0.196 ( 0.189)\tLoss 1.2082e+00 (1.4432e+00)\tAcc@1  73.44 ( 68.76)\tAcc@2  89.84 ( 81.87)\n",
      "Epoch: [3][2150/2163]\tTime  0.252 ( 0.249)\tData  0.193 ( 0.189)\tLoss 1.3396e+00 (1.4398e+00)\tAcc@1  69.53 ( 68.79)\tAcc@2  83.59 ( 81.90)\n",
      "Test: [  0/541]\tTime  0.215 ( 0.215)\tLoss 1.0079e+00 (1.0079e+00)\tAcc@1  76.56 ( 76.56)\tAcc@5  96.09 ( 96.09)\n",
      "Test: [ 50/541]\tTime  0.216 ( 0.222)\tLoss 1.3904e+00 (1.1552e+00)\tAcc@1  65.62 ( 72.26)\tAcc@5  90.62 ( 92.22)\n",
      "Test: [100/541]\tTime  0.219 ( 0.222)\tLoss 1.0681e+00 (1.1467e+00)\tAcc@1  71.88 ( 72.29)\tAcc@5  94.53 ( 92.42)\n",
      "Test: [150/541]\tTime  0.218 ( 0.230)\tLoss 1.0707e+00 (1.1481e+00)\tAcc@1  73.44 ( 72.38)\tAcc@5  92.97 ( 92.48)\n",
      "Test: [200/541]\tTime  0.223 ( 0.234)\tLoss 1.0278e+00 (1.1511e+00)\tAcc@1  74.22 ( 72.15)\tAcc@5  93.75 ( 92.43)\n",
      "Test: [250/541]\tTime  0.244 ( 0.232)\tLoss 1.2288e+00 (1.1547e+00)\tAcc@1  71.88 ( 72.05)\tAcc@5  90.62 ( 92.39)\n",
      "Test: [300/541]\tTime  0.219 ( 0.231)\tLoss 1.2215e+00 (1.1567e+00)\tAcc@1  66.41 ( 72.01)\tAcc@5  90.62 ( 92.41)\n",
      "Test: [350/541]\tTime  0.225 ( 0.229)\tLoss 1.0664e+00 (1.1561e+00)\tAcc@1  77.34 ( 72.00)\tAcc@5  92.19 ( 92.45)\n",
      "Test: [400/541]\tTime  0.229 ( 0.229)\tLoss 1.0977e+00 (1.1549e+00)\tAcc@1  75.00 ( 72.01)\tAcc@5  93.75 ( 92.49)\n",
      "Test: [450/541]\tTime  0.234 ( 0.228)\tLoss 1.1036e+00 (1.1558e+00)\tAcc@1  73.44 ( 71.97)\tAcc@5  95.31 ( 92.50)\n",
      "Test: [500/541]\tTime  0.218 ( 0.228)\tLoss 1.1426e+00 (1.1558e+00)\tAcc@1  73.44 ( 71.97)\tAcc@5  92.97 ( 92.50)\n",
      " * Acc@1 71.934 Acc@5 92.478\n",
      "Epoch: [4][   0/2163]\tTime  0.284 ( 0.284)\tData  0.210 ( 0.210)\tLoss 1.2680e+00 (1.2680e+00)\tAcc@1  72.66 ( 72.66)\tAcc@2  83.59 ( 83.59)\n",
      "Epoch: [4][  50/2163]\tTime  0.278 ( 0.261)\tData  0.211 ( 0.200)\tLoss 1.3409e+00 (1.2840e+00)\tAcc@1  69.53 ( 70.66)\tAcc@2  78.91 ( 83.30)\n",
      "Epoch: [4][ 100/2163]\tTime  0.267 ( 0.265)\tData  0.202 ( 0.203)\tLoss 1.2802e+00 (1.2818e+00)\tAcc@1  73.44 ( 70.47)\tAcc@2  84.38 ( 83.22)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataset = EuroCropsDataset(root=root,partition=partition,country=country)\n",
    "           \n",
    "fold_len = int(len(dataset) / 5)\n",
    "n_train = len(dataset) -  fold_len\n",
    "train_set, val_set =random_split(dataset,(n_train, fold_len),generator=torch.Generator().manual_seed(42))  \n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "pse_tae = PSE_TAE(device).to(device)\n",
    "\n",
    "focal_loss = FocalLoss(gamma).to(device)\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(pse_tae.parameters(), lr=learning_rate, betas=(b1, b2), weight_decay=weight_decay)\n",
    "\n",
    "# stops training when validation loss no longer decreases by min_delta after X epochs as defined by patience.\n",
    "early_stopper = EarlyStopper(patience=5, min_delta=0.2)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "pse_tae, optimizer, start_epoch = load_checkpoint(pse_tae, optimizer)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "\n",
    "    # ----------\n",
    "    #  Training\n",
    "    # ----------\n",
    "\n",
    "    train(train_loader,\n",
    "            pse_tae,\n",
    "            focal_loss,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            print_freq,\n",
    "            device)\n",
    "\n",
    "    # -----------\n",
    "    #  Validation\n",
    "    # -----------\n",
    "\n",
    "    acc1, val_loss = validate(val_loader,\n",
    "                    pse_tae,\n",
    "                    focal_loss,\n",
    "                    epoch,\n",
    "                    print_freq,\n",
    "                    device)\n",
    "\n",
    "    # -----------\n",
    "    #  Remember best acc@1 and save checkpoint\n",
    "    # -----------\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "    \n",
    "    if early_stopper.early_stop(val_loss):             \n",
    "        break\n",
    "\n",
    "    save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': pse_tae.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive R values from LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import h5py\n",
    "import joypy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/ayshahchan/Desktop/ESPACE/thesis/codes/XAI_thesis/train\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlps as mlps\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/Users/ayshahchan/Desktop/ESPACE/thesis/codes/XAI_thesis/models/best_checkpoint.pth.tar'\n",
    "\n",
    "\n",
    "data = dataset.data\n",
    "classes = dataset.crpgrpn\n",
    "crpNames = dataset.classes \n",
    "sample = data.loc[1].sort_index().fillna(0)\n",
    "sorteddf = data.sort_index(axis=1)\n",
    "date_values = pd.to_datetime(sample.index)\n",
    "\n",
    "\n",
    "dates_json = sample.index\n",
    "max_len = len(sample)\n",
    "# Instead of taking the position, the numbers of days since the first observation is used\n",
    "days = torch.zeros(max_len)\n",
    "date_0 = dates_json[0]\n",
    "date_0 = datetime.datetime.strptime(str(date_0), \"%Y-%m-%d\")\n",
    "days[0] = 0\n",
    "for i in range(max_len - 1):\n",
    "    date = dates_json[i + 1]\n",
    "    date = datetime.datetime.strptime(str(date), \"%Y-%m-%d\")\n",
    "    days[i + 1] = (date - date_0).days\n",
    "days = days.unsqueeze(1)\n",
    "days = days.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = PSE_TAE(device)\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=1e-5, betas=(0.9, 0.999),weight_decay=1e-6)\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['best_acc1']\n",
    "model.to(device)\n",
    "Rdict = []\n",
    "iddict = []\n",
    "crpdict = []\n",
    "cropNames = []\n",
    "predictid = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "     \n",
    "    x = batch['data'].to(device)\n",
    "    x_label = batch['label'].to(device)\n",
    "    # days = batch[\"dates\"].to(device)\n",
    "    \n",
    "    x_with_grad = torch.autograd.Variable(x, requires_grad=True)\n",
    "    parcel_id = batch['ids'].squeeze().detach().numpy()\n",
    "    \n",
    "    \n",
    "    crop_name = batch['crop name'][0]\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "\n",
    "    outs = model.forward_and_explain(x_with_grad, x_label, days)\n",
    "    attribution = outs['R'].squeeze()\n",
    "    predictid.append(outs['logits'].argmax().squeeze())\n",
    "    Rdict.append(attribution)\n",
    "    iddict.append(parcel_id)\n",
    "    crpdict.append(x_label.cpu().numpy())\n",
    "    cropNames.append(crop_name)\n",
    "\n",
    "\n",
    "Rdict = np.array(Rdict)\n",
    "predictid = np.array(torch.tensor(predictid).cpu())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample code to plot the relevance scores at each timestep and the corresponding band-specific relevance scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZING_FACTOR = 1e-4\n",
    "BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8','B9','B10', 'B11', 'B12',\n",
    "       'B8A']\n",
    "\n",
    "parcel_id = 113\n",
    "dict_id = np.where(iddict==parcel_id)[0].item()\n",
    "attribution = Rdict[dict_id].squeeze()\n",
    "dataval = sorteddf.loc[parcel_id]\n",
    "print(cropNames[dict_id])\n",
    "# fill missing arrays with 0 and normalize\n",
    "a = [np.array(ii) for ii in dataval ] \n",
    "x = np.empty([len(a), 13])\n",
    "for ii in range(len(a)):\n",
    "    if a[ii] is None or np.all(a[ii] == None):\n",
    "        x[ii,:] =  np.zeros(13)\n",
    "    else:\n",
    "        x[ii,:] = np.array(a[ii]* NORMALIZING_FACTOR)\n",
    "x = np.nan_to_num(x)\n",
    "\n",
    "\n",
    "r_df = pd.DataFrame(attribution, columns=BANDS, index = date_values)\n",
    "# change date range according to data\n",
    "idx = pd.date_range('01-06-2019', '12-27-2019')\n",
    "test = r_df.reindex(idx, fill_value=0) \n",
    "x_range = list(range(len(test)))\n",
    "fig, axes = joypy.joyplot(test,overlap=2, kind=\"values\", x_range=x_range, linecolor=\"black\", colormap=cm.tab20,\n",
    "grid=True, figsize=(10,8))\n",
    "xtick_range = [datetime.date(2021, y, 1).timetuple().tm_yday-6 for y in range(2,13)]\n",
    "xlabel = [ y.isoformat()[0:7] for y in test.index.date[xtick_range]]\n",
    "axes[-1].set_xticks(xtick_range)\n",
    "axes[-1].set_xticklabels(xlabel)\n",
    "\n",
    "test = np.sum(attribution,axis=1)\n",
    "center_function = lambda x: x - np.nanmean(x)\n",
    "normR =(test - min(test)) / ( max(test) - min(test) ) -0.5\n",
    "centeredR = center_function(normR)*2\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2,1, figsize=(12,8))\n",
    "\n",
    "colors = cm.tab20(np.linspace(0, 1, len(attribution.T)))\n",
    "for z, c in zip(x.T, colors):\n",
    "    axs[0].plot(date_values, z,'o-',color=c)\n",
    "axs[0].set_ylabel(\"Input\", fontsize=18)\n",
    "axs[0].set_xlim([date_values[0], date_values[len(date_values)-1]])\n",
    "\n",
    "axs[0].legend(BANDS)\n",
    "axs[1].grid()\n",
    "axs[1].plot(date_values, centeredR, 'o-',color='black')\n",
    "axs[1].set_ylabel(\"R\", fontsize=18)\n",
    "#axs[1].set_ylim([-1, 1])\n",
    "axs[1].set_xlim([date_values[0], date_values[len(date_values)-1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eurocrops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
